/******/ // The require scope
/******/ var __webpack_require__ = {};
/******/ 
/************************************************************************/
/******/ /* webpack/runtime/define property getters */
/******/ (() => {
/******/ 	// define getter functions for harmony exports
/******/ 	__webpack_require__.d = (exports, definition) => {
/******/ 		for(var key in definition) {
/******/ 			if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 				Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 			}
/******/ 		}
/******/ 	};
/******/ })();
/******/ 
/******/ /* webpack/runtime/hasOwnProperty shorthand */
/******/ (() => {
/******/ 	__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ })();
/******/ 
/************************************************************************/
var __webpack_exports__ = {};
/* harmony export */ __webpack_require__.d(__webpack_exports__, {
/* harmony export */   Z: () => (__WEBPACK_DEFAULT_EXPORT__)
/* harmony export */ });
/* harmony default export */ const __WEBPACK_DEFAULT_EXPORT__ = ([{"id":"TheBloke/Airoboros-L2-13B-2.1-GGUF","name":"Airoboros-L2-13B-2.1-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"wizardcoder-python-13b-v1.0.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":5430000000,"maxRamRequired":7930000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q2_K.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":6930000000,"maxRamRequired":9430000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q3_K_L.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":7870000000,"maxRamRequired":10370000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q4_K_M.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":9230000000,"maxRamRequired":11730000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q5_K_M.gguf"}]},{"id":"TheBloke/Chinese-Llama-2-13B-GGUF","name":"Chinese-Llama-2-13B-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/Chinese-Llama-2-13B-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"chinese-llama-2-13b.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":5570000000,"maxRamRequired":8070000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/Chinese-Llama-2-13B-GGUF/resolve/main/chinese-llama-2-13b.Q2_K.gguf"},{"name":"chinese-llama-2-13b.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":7080000000,"maxRamRequired":9580000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/Chinese-Llama-2-13B-GGUF/resolve/main/chinese-llama-2-13b.Q3_K_L.gguf"},{"name":"chinese-llama-2-13b.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":8030000000,"maxRamRequired":10530000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/Chinese-Llama-2-13B-GGUF/resolve/main/chinese-llama-2-13b.Q4_K_M.gguf"},{"name":"chinese-llama-2-13b.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":39410000000,"maxRamRequired":11910000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/Chinese-Llama-2-13B-GGUF/resolve/main/chinese-llama-2-13b.Q5_K_M.gguf"}]},{"id":"TheBloke/Falcon-180B-Chat-GGUF","name":"Falcon-180B-Chat-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"falcon-180b-chat.Q2_K.gguf-split-a","quantMethod":"Q2_K","bits":2,"size":73970000000,"maxRamRequired":76470000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF/resolve/main/falcon-180b-chat.Q2_K.gguf-split-a"},{"name":"falcon-180b-chat.Q3_K_L.gguf-split-a","quantMethod":"Q3_K_L","bits":3,"size":91990000000,"maxRamRequired":94490000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF/resolve/main/falcon-180b-chat.Q3_K_L.gguf-split-a"},{"name":"falcon-180b-chat.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":101480000000,"maxRamRequired":110980000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF/resolve/main/falcon-180b-chat.Q4_K_M.gguf-split-a"},{"name":"falcon-180b-chat.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":139990000000,"maxRamRequired":133490000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/Falcon-180B-Chat-GGUF/resolve/main/falcon-180b-chat.Q5_K_M.gguf-split-c"}]},{"id":"TheBloke/Mistral-7B-Instruct-v0.1-GGUF","name":"Mistral-7B-Instruct-v0.1-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"mistral-7b-instruct-v0.1.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":3080000000,"maxRamRequired":5580000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q2_K.gguf"},{"name":"mistral-7b-instruct-v0.1.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":3820000000,"maxRamRequired":6320000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_L.gguf"},{"name":"mistral-7b-instruct-v0.1.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":4370000000,"maxRamRequired":6870000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"},{"name":"mistral-7b-instruct-v0.1.Q5_K_S.gguf","quantMethod":"Q5","bits":5,"size":5000000000,"maxRamRequired":7500000000,"usecase":"large, low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_S.gguf"},{"name":"mistral-7b-instruct-v0.1.Q5_K_S.gguf","quantMethod":"Q6_K","bits":6,"size":5940000000,"maxRamRequired":8440000000,"usecase":"very large, extremely low quality loss","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf"},{"name":"mistral-7b-instruct-v0.1.Q8_0.gguf","quantMethod":"Q8_0","bits":8,"size":7700000000,"maxRamRequired":10020000000,"usecase":"very large, extremely low quality loss - not recommended","downloadLink":"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf"}]},{"id":"TheBloke/OpenHermes-2-Mistral-7B-GGUF","name":"OpenHermes-2-Mistral-7B-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"openhermes-2-mistral-7b.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":3080000000,"maxRamRequired":5580000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q2_K.gguf"},{"name":"openhermes-2-mistral-7b.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":3820000000,"maxRamRequired":6320000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q3_K_L.gguf"},{"name":"openhermes-2-mistral-7b.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":4370000000,"maxRamRequired":6870000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q4_K_M.gguf"},{"name":"openhermes-2-mistral-7b.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":5130000000,"maxRamRequired":7630000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/OpenHermes-2-Mistral-7B-GGUF/resolve/main/openhermes-2-mistral-7b.Q5_K_M.gguf"}]},{"id":"TheBloke/WizardCoder-Python-13B-V1.0-GGUF","name":"WizardCoder-Python-13B-V1.0-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"wizardcoder-python-13b-v1.0.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":5430000000,"maxRamRequired":7930000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q2_K.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":6930000000,"maxRamRequired":9430000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q3_K_L.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":7870000000,"maxRamRequired":10370000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q4_K_M.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":9230000000,"maxRamRequired":11730000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q5_K_M.gguf"}]},{"id":"TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF","name":"llama-2-13B-Guanaco-QLoRA-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"llama-2-13b-guanaco-qlora.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":5430000000,"maxRamRequired":7430000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF/resolve/main/llama-2-13b-guanaco-qlora.Q2_K.gguf"},{"name":"llama-2-13b-guanaco-qlora.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":6340000000,"maxRamRequired":8160000000,"usecase":"very small, high quality loss","downloadLink":"https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF/resolve/main/llama-2-13b-guanaco-qlora.Q3_K_L.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":7870000000,"maxRamRequired":10370000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF/resolve/main/llama-2-13b-guanaco-qlora.Q4_K_M.gguf"},{"name":"wizardcoder-python-13b-v1.0.Q4_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":9230000000,"maxRamRequired":11730000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGUF/resolve/main/llama-2-13b-guanaco-qlora.Q5_K_M.gguf"}]},{"id":"TheBloke/zephyr-7B-alpha-GGUF","name":"zephyr-7B-alpha-GGUF","shortDescription":"some description","avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"The Bloke","version":"1.0.0","modelUrl":"https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"airoboros-l2-13b-2.1.Q2_K.gguf","quantMethod":"Q2_K","bits":2,"size":5430000000,"maxRamRequired":7930000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF/resolve/main/airoboros-l2-13b-2.1.Q2_K.gguf"},{"name":"airoboros-l2-13b-2.1.Q3_K_L.gguf","quantMethod":"Q3_K_L","bits":3,"size":6930000000,"maxRamRequired":9430000000,"usecase":"small, substantial quality loss","downloadLink":"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF/resolve/main/airoboros-l2-13b-2.1.Q3_K_L.gguf"},{"name":"airoboros-l2-13b-2.1.Q4_K_M.gguf","quantMethod":"Q4_K_M","bits":4,"size":7870000000,"maxRamRequired":10370000000,"usecase":"medium, balanced quality - recommended","downloadLink":"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF/resolve/main/airoboros-l2-13b-2.1.Q4_K_M.gguf"},{"name":"airoboros-l2-13b-2.1.Q5_K_M.gguf","quantMethod":"Q5_K_M","bits":5,"size":9230000000,"maxRamRequired":11730000000,"usecase":"large, very low quality loss - recommended","downloadLink":"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF/resolve/main/airoboros-l2-13b-2.1.Q5_K_M.gguf"}]},{"id":"bhlim/llama-2-chat-GGUF","name":"llama-2-chat-GGUF","shortDescription":"some description","avatarUrl":"","longDescription":"GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.","author":"bhlim","version":"1.0.0","modelUrl":"bhlim/llama-2-chat-GGUF","nsfw":false,"tags":["freeform","tags"],"defaultGreeting":"Hello there","type":"LLM","createdAt":0,"versions":[{"name":"llama-2-13b-chat.gguf.q4_0.bin","quantMethod":"","bits":2,"size":7859790151,"maxRamRequired":5580000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/bhlim/llama-2-chat-GGUF/resolve/main/llama-2-13b-chat.gguf.q4_0.bin"},{"name":"llama-2-7b-chat.gguf.q4_0.bin","quantMethod":"","bits":2,"size":4069481512,"maxRamRequired":5580000000,"usecase":"smallest, significant quality loss - not recommended for most purposes","downloadLink":"https://huggingface.co/bhlim/llama-2-chat-GGUF/resolve/main/llama-2-7b-chat.gguf.q4_0.bin"}]}]);

var __webpack_exports__default = __webpack_exports__.Z;
export { __webpack_exports__default as default };
