name: Convert LLaVA model to GGUF (Hardcoded Test)

on:
  push:
    branches: [ci/add-llava-conversion-pipeline]
  workflow_dispatch:

env:
  USER_NAME: cortexso
  LLAVA_MODEL_ID: liuhaotian/llava-v1.6-mistral-7b
  LLAVA_VERSION: 1.6
  TARGET_MODEL_ID: llava
  QUANT_LEVEL: q4-km

jobs:
  converter:
    runs-on: ubuntu-20-04-gguf
    timeout-minutes: 7200
    steps:
      - name: Checkout llama.cpp repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          repository: ggml-org/llama.cpp

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache Python packages
        uses: actions/cache@0c45773b623bea8c8e75f6c82b208c3cf94ea4f9
        with:
          path: |
            ~/.cache/pip
            ~/.local/share/pip
            .venv
          key: ${{ runner.os }}-pip-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip3 install -r requirements.txt
          pip3 install -r examples/llava/requirements.txt
          pip3 install hf-transfer
          pip3 install torch
          git lfs install

      - name: Extract MODEL_NAME
        run: |
          MODEL_NAME="$(echo ${{ env.LLAVA_MODEL_ID }} | rev | cut -d/ -f1 | rev)"
          echo $MODEL_NAME
          MODEL_NAME="$(echo $MODEL_NAME | tr '[:upper:]' '[:lower:]')"
          echo $MODEL_NAME
          echo "MODEL_NAME=$MODEL_NAME" >> $GITHUB_ENV

      - name: Print environment variables
        run: |
          echo "LLAVA_MODEL_ID: ${{ env.LLAVA_MODEL_ID }}"
          echo "MODEL_NAME: ${{ env.MODEL_NAME }}"
          echo "LLAVA_VERSION: ${{ env.LLAVA_VERSION }}"

      - name: Prepare folders
        run: |
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/hf
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/vit
          mkdir -p /mnt/models/.cache

      - name: Download LLaVA model
        uses: nick-fields/retry@v2
        with:
          timeout_minutes: 15
          max_attempts: 5
          command: |
            HF_HUB_ETAG_TIMEOUT=500 huggingface-cli download --repo-type model --local-dir /mnt/models/${{ env.MODEL_NAME }}/hf --cache-dir /mnt/models/.cache --token ${{ secrets.HUGGINGFACE_TOKEN_READ }} ${{ env.LLAVA_MODEL_ID }}

      - name: Perform LLaVA surgery
        run: |
          python ./examples/llava/llava_surgery_v2.py -C -m /mnt/models/${{ env.MODEL_NAME }}/hf
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/vit
          cp /mnt/models/${{ env.MODEL_NAME }}/hf/llava.clip /mnt/models/${{ env.MODEL_NAME }}/vit/pytorch_model.bin
          cp /mnt/models/${{ env.MODEL_NAME }}/hf/llava.projector /mnt/models/${{ env.MODEL_NAME }}/vit/
          # Download config for ViT
          curl -s -q https://huggingface.co/cmp-nct/llava-1.6-gguf/raw/main/config_vit.json -o /mnt/models/${{ env.MODEL_NAME }}/vit/config.json

      - name: Convert image encoder to GGUF
        run: |
          # For LLaVA 1.6, use the surgery output directly
          python ./examples/llava/convert_image_encoder_to_gguf.py -m /mnt/models/${{ env.MODEL_NAME }}/vit --llava-projector /mnt/models/${{ env.MODEL_NAME }}/vit/llava.projector --output-dir /mnt/models/${{ env.MODEL_NAME }}/vit --clip-model-is-vision

      - name: Build llama.cpp
        run: |
          cmake -B build
          cmake --build build --config Release

      - name: Convert LLaMA part to GGUF
        run: |
          python ./examples/convert_legacy_llama.py /mnt/models/${{ env.MODEL_NAME }}/hf --skip-unknown || (
            echo "Legacy conversion failed, trying HF conversion method..."
            # If legacy conversion fails, try converting using the HF converter
            python ./convert_hf_to_gguf.py /mnt/models/${{ env.MODEL_NAME }}/hf --outfile /mnt/models/${{ env.MODEL_NAME }}/hf/ggml-model-f16.gguf
          )
          
          # List files to see what was created
          echo "Listing files in the output directory:"
          ls -la /mnt/models/${{ env.MODEL_NAME }}/hf/
          
          # Find the gguf file and copy it to the expected name if needed
          GGUF_FILE=$(find /mnt/models/${{ env.MODEL_NAME }}/hf/ -name "*.gguf" | head -n 1)
          if [ -n "$GGUF_FILE" ] && [ "$GGUF_FILE" != "/mnt/models/${{ env.MODEL_NAME }}/hf/ggml-model-f16.gguf" ]; then
            echo "Found GGUF file at $GGUF_FILE, copying to expected location"
            cp "$GGUF_FILE" /mnt/models/${{ env.MODEL_NAME }}/hf/ggml-model-f16.gguf
          fi

      - name: Quantize the model
        run: |
          # Set appropriate paths for LLaVA 1.6
          MMPROJ_PATH="/mnt/models/${{ env.MODEL_NAME }}/vit/mmproj-model-f16.gguf"
          
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/

          # Create directory for base f16 model
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/f16/
          cp /mnt/models/${{ env.MODEL_NAME }}/hf/ggml-model-f16.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/f16/model.gguf
          cp ${MMPROJ_PATH} /mnt/models/${{ env.MODEL_NAME }}/gguf/f16/mmproj.gguf

          # Quantize for q4-km
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/
          ./build/bin/llama-quantize /mnt/models/${{ env.MODEL_NAME }}/hf/ggml-model-f16.gguf /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/model.gguf Q4_K_M
          cp ${MMPROJ_PATH} /mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/mmproj.gguf

      - name: Upload to Hugging Face (quantization branches)
        run: |
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN_WRITE }} --add-to-git-credential
          
          # Upload f16 model
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/f16/" . --revision "gguf-f16"
          
          # Upload q4-km model
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" "/mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/" . --revision "gguf-q4-km"

      - name: Upload to Hugging Face (main branch)
        run: |
          # Create a temporary directory for renamed files
          mkdir -p /mnt/models/${{ env.MODEL_NAME }}/gguf/main

          # Copy mmproj file
          MMPROJ_PATH="/mnt/models/${{ env.MODEL_NAME }}/vit/mmproj-model-f16.gguf"
          cp ${MMPROJ_PATH} /mnt/models/${{ env.MODEL_NAME }}/gguf/main/mmproj-model-f16.gguf

          # Copy and rename files with simplified logic
          cp "/mnt/models/${{ env.MODEL_NAME }}/gguf/f16/model.gguf" \
             "/mnt/models/${{ env.MODEL_NAME }}/gguf/main/${{ env.MODEL_NAME }}-f16.gguf"
          
          cp "/mnt/models/${{ env.MODEL_NAME }}/gguf/q4-km/model.gguf" \
             "/mnt/models/${{ env.MODEL_NAME }}/gguf/main/${{ env.MODEL_NAME }}-q4_k_m.gguf"

          # Upload to main branch
          huggingface-cli upload "${{ env.USER_NAME }}/${{ env.TARGET_MODEL_ID }}" \
                                "/mnt/models/${{ env.MODEL_NAME }}/gguf/main/" \
                                . \
                                --revision "main"

          # Cleanup
          huggingface-cli logout