{
  "id": "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
  "name": "Mistral-7B-Instruct-v0.1-GGUF",
  "shortDescription": "some description",
  "avatarUrl": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face",
  "longDescription": "GGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp. GGUF offers numerous advantages over GGML, such as better tokenisation, and support for special tokens. It is also supports metadata, and is designed to be extensible.",
  "author": "The Bloke",
  "version": "1.0.0",
  "modelUrl": "https://google.com",
  "nsfw": false,
  "tags": ["freeform", "tags"],
  "defaultGreeting": "Hello there",
  "type": "LLM",
  "createdAt": 0,
  "versions": [
    {
      "name": "mistral-7b-instruct-v0.1.Q2_K.gguf",
      "quantMethod": "Q2_K",
      "bits": 2,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q2_K.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q3_K_L.gguf",
      "quantMethod": "Q2_K",
      "bits": 2,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_L.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q3_K_M.gguf",
      "quantMethod": "Q3_K_M",
      "bits": 3,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q3_K_M.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q4_0.gguf",
      "quantMethod": "Q4",
      "bits": 4,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_0.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q4_K_M.gguf",
      "quantMethod": "Q4_K_M",
      "bits": 4,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q5_0.gguf",
      "quantMethod": "Q5",
      "bits": 5,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_0.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q5_0.gguf",
      "quantMethod": "Q6_K",
      "bits": 6,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf"
    },
    {
      "name": "mistral-7b-instruct-v0.1.Q8_0.gguf",
      "quantMethod": "Q8",
      "bits": 8,
      "size": 3080000000,
      "maxRamRequired": 5580000000,
      "usecase": "smallest, significant quality loss - not recommended for most purposes",
      "downloadLink": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf"
    }
  ]
}
